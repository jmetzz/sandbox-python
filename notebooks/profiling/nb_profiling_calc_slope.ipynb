{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba optimization\n",
    "\n",
    "Numba works by generating optimized machine code using the LLVM compiler infrastructure at import time, runtime, or statically (using the included pycc tool). Numba supports compilation of Python to run on either CPU or GPU hardware, and is designed to integrate with the Python scientific software stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from C.E.\n",
    "def calc_slope(data: pd.DataFrame, key_columns: List[str], slope_column: str, fact_name: str) -> pd.DataFrame:\n",
    "    if not data.empty:\n",
    "        pdf_with_slope = data.sort_values(by=\"period_seq\").groupby(key_columns).apply(slope, slope_column).reset_index()\n",
    "        if pdf_with_slope.empty:\n",
    "            data[fact_name] = 0.0\n",
    "        else:\n",
    "            pdf_with_slope.columns = key_columns + [fact_name]\n",
    "            data = data.merge(pdf_with_slope, on=key_columns)\n",
    "            return data\n",
    "    else:\n",
    "        return pd.DataFrame(columns=data.columns.to_list() + [fact_name])\n",
    "\n",
    "\n",
    "def slope(data: pd.DataFrame, sales_column: str):\n",
    "    num_periods = list(range(data.shape[0]))\n",
    "    sales = data[sales_column].to_numpy()\n",
    "\n",
    "    sum_num_periods = sum(num_periods)\n",
    "    sum_sales = sum(sales)\n",
    "\n",
    "    sum_num_periods_sales = np.dot(num_periods, sales)\n",
    "    sum_num_periods_square = np.dot(num_periods, num_periods)\n",
    "\n",
    "    square_of_sum_num_periods = sum_num_periods * sum_num_periods\n",
    "    slope_num = len(num_periods) * sum_num_periods_sales - sum_num_periods * sum_sales\n",
    "    slope_den = len(num_periods) * sum_num_periods_square - square_of_sum_num_periods\n",
    "\n",
    "    return slope_num / slope_den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CE we use `calc_slope` function twice inside the rule_53 implementation.\n",
    "\n",
    "```python\n",
    "output_df = calc_slope(output_df, ['item_id', 'competitor_item_id'], 'salesunits_own', 'slope_own')\n",
    "#...\n",
    "output_df = calc_slope(output_df, ['item_id', 'competitor_item_id'], 'salesunits_competitor', 'slope_competitor')\n",
    "```\n",
    "\n",
    "Let's check how expensive this function is, and if we can improve the execution time with numba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare a sample data frame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#    We need the cell's data and configuration as they are used in CE:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m configuration \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_group_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTV_FLAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     },\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPTV_FLAT-CN.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/workspace/sandbox-python/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/sandbox-python/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[1;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[1;32m   1029\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/sandbox-python/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/sandbox-python/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/sandbox-python/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:1400\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1396\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1400\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m     )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1403\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1405\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1406\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Prepare a sample data frame\n",
    "#    We need the cell's data and configuration as they are used in CE:\n",
    "\n",
    "configuration = {\n",
    "    \"country_code\": \"CN\",\n",
    "    \"item_group_code\": \"PTV_FLAT\",\n",
    "    \"market_configuration\": {\n",
    "        \"ce\": {\n",
    "            \"low_price_percentage\": 0.1,\n",
    "            \"high_price_percentage\": 0.1,\n",
    "            \"medium_price_percentage\": 0.1,\n",
    "            \"lower_price_range_threshold\": 0,\n",
    "            \"upper_price_range_threshold\": 999999999,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "df = pd.read_json(io.StringIO(\"PTV_FLAT-CN.py\"), orient=\"columns\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_slope(df, [\"item_id\", \"competitor_item_id\"], \"salesunits_own\", \"slope_own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit calc_slope(df, ['item_id', 'competitor_item_id'], 'salesunits_own', 'slope_own')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a look and see where the time is spent during this operation (limited to the most time consuming four calls) using the prun ipython magic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -l 4 calc_slope(df, ['item_id', 'competitor_item_id'], 'salesunits_own', 'slope_own')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "\n",
    "> NOTE: As of Numba version 0.20, pandas objects cannot be passed directly to Numba-compiled functions. Instead, one must pass the NumPy array underlying the pandas object to the Numba-compiled function.\n",
    "\n",
    "\n",
    "\n",
    "- Pandas is not understood by Numba and as a result Numba would simply run the code via the interpreter but with the added cost of the Numba internal overheads!\n",
    "- To use JIT compile with Numba, we need to write code based on vectorizationa & broadcasting technique.\n",
    "- Vectorizing the code only plays well with Numpy and simple Python syntax.\n",
    "- Instead of using a Pandas `apply`, separate out numerical calculations into a Numba sub-function.\n",
    "- `pyyaml` - enables configuration of Numba via a YAML config file.\n",
    "- The parallel option for jit() can produce diagnostic information about the transforms undertaken in automatically parallelizing the decorated code. This information can be accessed in two ways, the first is by setting the environment variable `NUMBA_PARALLEL_DIAGNOSTICS`, the second is by calling `parallel_diagnostics()`, both methods give the same information and print to STDOUT. The level of verbosity in the diagnostic information is controlled by an integer argument of value between 1 and 4 inclusive, 1 being the least verbose and 4 the most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def f_plain(x):\n",
    "    return x * (x - 1)\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def integrate_f_numba(a, b, N):\n",
    "    s = 0\n",
    "    dx = (b - a) / N\n",
    "    for i in range(N):\n",
    "        s += f_plain(a + i * dx)\n",
    "    return s * dx\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def apply_integrate_f_numba(col_a, col_b, col_N):\n",
    "    #     n = len(col_N)\n",
    "    n = col_N.shape[0]\n",
    "    result = np.empty(n, dtype=np.float64)\n",
    "    assert len(col_a) == len(col_b) == n\n",
    "    for i in range(n):\n",
    "        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_numba(df):\n",
    "    result = apply_integrate_f_numba(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n",
    "    return pd.Series(result, index=df.index, name=\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = pd.DataFrame(\n",
    "    {\"a\": np.random.randn(1000), \"b\": np.random.randn(1000), \"N\": np.random.randint(100, 1000, (1000)), \"x\": \"x\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_numba(rand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numba optimized code\n",
    "def calc_optimized_slope(data: pd.DataFrame, key_columns: List[str], slope_column: str, fact_name: str) -> pd.DataFrame:\n",
    "    if not data.empty:\n",
    "        pdf_with_slope = (\n",
    "            data.sort_values(by=\"period_seq\")\n",
    "            .groupby(key_columns)[slope_column]\n",
    "            .apply(apply_optimized_slope, raw=True)\n",
    "            .reset_index()\n",
    "        )\n",
    "        if pdf_with_slope.empty:\n",
    "            data[fact_name] = 0.0\n",
    "        else:\n",
    "            pdf_with_slope.columns = key_columns + [fact_name]\n",
    "            data = data.merge(pdf_with_slope, on=key_columns)\n",
    "            return data\n",
    "    else:\n",
    "        return pd.DataFrame(columns=data.columns.to_list() + [fact_name])\n",
    "\n",
    "\n",
    "# def apply_optimized_slope(data: np.ndarray, sales_column: str):\n",
    "#     return the_real_calculation(data[sales_column].to_numpy(),\n",
    "#                                 np.array(data.shape[0]))\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def apply_optimized_slope(sales: np.ndarray) -> float:\n",
    "    num_periods = np.arange(float(size))\n",
    "    print(size)\n",
    "    print(sales.shape)\n",
    "    print(sales)\n",
    "\n",
    "    #     sum_num_periods = np.sum(num_periods)\n",
    "    #     sum_sales = np.sum(sales)\n",
    "\n",
    "    #     sum_num_periods_sales = np.dot(num_periods, sales)\n",
    "    #     sum_num_periods_square = np.dot(num_periods, num_periods)\n",
    "\n",
    "    #     square_of_sum_num_periods = sum_num_periods * sum_num_periods\n",
    "    #     slope_num = size * sum_num_periods_sales - sum_num_periods * sum_sales\n",
    "    #     slope_den = size * sum_num_periods_square - square_of_sum_num_periods\n",
    "\n",
    "    #     square_of_sum_num_periods = np.multiply(sum_num_periods, sum_num_periods)\n",
    "    #     slope_num = np.subtract((size * sum_num_periods_sales), (sum_num_periods * sum_sales))\n",
    "    #     slope_den = np.subtract((size * sum_num_periods_square), square_of_sum_num_periods)\n",
    "\n",
    "    #     return np.divide(slope_num, slope_den)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_optimized_slope(df, [\"item_id\", \"competitor_item_id\"], \"salesunits_own\", \"slope_own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit calc_optimized_slope(df, ['item_id', 'competitor_item_id'], 'salesunits_own', 'slope_own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
